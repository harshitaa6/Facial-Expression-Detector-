{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7af619a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Deep Learning Libraries\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense,,Dropout,,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a82fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [48,48]\n",
    "batch_size  = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aedef25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training & validation dataset\n",
    "train_data_dir = r'C:\\Users\\harsh\\Documents\\expression\\images\\train'\n",
    "validation_data_dir = r'C:\\Users\\harsh\\Documents\\expression\\images\\validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accfc9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data augmentation \n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=30,\n",
    "                                   shear_range=0.3,\n",
    "                                   zoom_range=0.3,\n",
    "                                   width_shift_range=0.4,\n",
    "                                   height_shift_range=0.4,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest'\n",
    "                                   )\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f760274c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28821 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_set = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                    color_mode='grayscale',\n",
    "                                                    target_size=IMAGE_SIZE,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae7ce886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7066 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "val_set = validation_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                              color_mode='grayscale',\n",
    "                                                              target_size=IMAGE_SIZE,\n",
    "                                                              batch_size=batch_size,\n",
    "                                                              class_mode='categorical',\n",
    "                                                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6ae7b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 512)       590336    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 3591      \n",
      "=================================================================\n",
      "Total params: 4,478,727\n",
      "Trainable params: 4,474,759\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
    "\n",
    "no_of_classes = 7\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#1st CNN layer\n",
    "model.add(Conv2D(64,(3,3),padding = 'same',input_shape = (48,48,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#2nd CNN layer\n",
    "model.add(Conv2D(128,(5,5),padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout (0.25))\n",
    "\n",
    "#3rd CNN layer\n",
    "model.add(Conv2D(512,(3,3),padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout (0.25))\n",
    "\n",
    "#4th CNN layer\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#Fully connected 1st layer\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# Fully connected layer 2nd layer\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(no_of_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "opt = Adam(lr = 0.0001)#learning rate\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00050623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop,SGD,Adam\n",
    "\n",
    "epochs = 48\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66016c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-0ec4e923e39c>:5: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/48\n",
      "225/225 [==============================] - 1048s 5s/step - loss: 1.9492 - accuracy: 0.2138 - val_loss: 1.8874 - val_accuracy: 0.2581\n",
      "Epoch 2/48\n",
      "225/225 [==============================] - 970s 4s/step - loss: 1.8573 - accuracy: 0.2313 - val_loss: 1.8583 - val_accuracy: 0.2580\n",
      "Epoch 3/48\n",
      "225/225 [==============================] - 952s 4s/step - loss: 1.8239 - accuracy: 0.2401 - val_loss: 1.8332 - val_accuracy: 0.2589\n",
      "Epoch 4/48\n",
      "225/225 [==============================] - 691s 3s/step - loss: 1.8075 - accuracy: 0.2459 - val_loss: 1.7592 - val_accuracy: 0.2811\n",
      "Epoch 5/48\n",
      "225/225 [==============================] - 802s 4s/step - loss: 1.7947 - accuracy: 0.2551 - val_loss: 1.8477 - val_accuracy: 0.2642\n",
      "Epoch 6/48\n",
      "225/225 [==============================] - 711s 3s/step - loss: 1.7704 - accuracy: 0.2671 - val_loss: 1.7111 - val_accuracy: 0.3034\n",
      "Epoch 7/48\n",
      "225/225 [==============================] - 702s 3s/step - loss: 1.7309 - accuracy: 0.2918 - val_loss: 1.6197 - val_accuracy: 0.3452\n",
      "Epoch 8/48\n",
      "225/225 [==============================] - 692s 3s/step - loss: 1.6876 - accuracy: 0.3196 - val_loss: 1.7186 - val_accuracy: 0.2486\n",
      "Epoch 9/48\n",
      "225/225 [==============================] - 686s 3s/step - loss: 1.6455 - accuracy: 0.3426 - val_loss: 2.4201 - val_accuracy: 0.2766\n",
      "Epoch 10/48\n",
      "225/225 [==============================] - 676s 3s/step - loss: 1.6103 - accuracy: 0.3627 - val_loss: 1.6024 - val_accuracy: 0.4084\n",
      "Epoch 11/48\n",
      "225/225 [==============================] - 688s 3s/step - loss: 1.5664 - accuracy: 0.3840 - val_loss: 1.4330 - val_accuracy: 0.4429\n",
      "Epoch 12/48\n",
      "225/225 [==============================] - 688s 3s/step - loss: 1.5384 - accuracy: 0.4001 - val_loss: 1.3992 - val_accuracy: 0.4668\n",
      "Epoch 13/48\n",
      "225/225 [==============================] - 676s 3s/step - loss: 1.5056 - accuracy: 0.4105 - val_loss: 1.3937 - val_accuracy: 0.4554\n",
      "Epoch 14/48\n",
      "225/225 [==============================] - 688s 3s/step - loss: 1.4855 - accuracy: 0.4191 - val_loss: 1.4895 - val_accuracy: 0.4229\n",
      "Epoch 15/48\n",
      "225/225 [==============================] - 695s 3s/step - loss: 1.4558 - accuracy: 0.4379 - val_loss: 1.4431 - val_accuracy: 0.4528\n",
      "Epoch 16/48\n",
      "225/225 [==============================] - 680s 3s/step - loss: 1.4494 - accuracy: 0.4392 - val_loss: 1.7167 - val_accuracy: 0.2930\n",
      "Epoch 17/48\n",
      "225/225 [==============================] - 682s 3s/step - loss: 1.4247 - accuracy: 0.4489 - val_loss: 1.5602 - val_accuracy: 0.4013\n",
      "Epoch 18/48\n",
      "225/225 [==============================] - 694s 3s/step - loss: 1.4085 - accuracy: 0.4563 - val_loss: 1.2909 - val_accuracy: 0.4966\n",
      "Epoch 19/48\n",
      "225/225 [==============================] - 703s 3s/step - loss: 1.3959 - accuracy: 0.4614 - val_loss: 1.3360 - val_accuracy: 0.4749\n",
      "Epoch 20/48\n",
      "225/225 [==============================] - 733s 3s/step - loss: 1.3866 - accuracy: 0.4675 - val_loss: 1.2335 - val_accuracy: 0.5298\n",
      "Epoch 21/48\n",
      "225/225 [==============================] - 709s 3s/step - loss: 1.3634 - accuracy: 0.4759 - val_loss: 1.2344 - val_accuracy: 0.5376\n",
      "Epoch 22/48\n",
      "225/225 [==============================] - 702s 3s/step - loss: 1.3547 - accuracy: 0.4764 - val_loss: 1.3014 - val_accuracy: 0.4888\n",
      "Epoch 23/48\n",
      "225/225 [==============================] - 797s 4s/step - loss: 1.3472 - accuracy: 0.4805 - val_loss: 1.1582 - val_accuracy: 0.5591\n",
      "Epoch 24/48\n",
      "225/225 [==============================] - 719s 3s/step - loss: 1.3333 - accuracy: 0.4899 - val_loss: 1.4004 - val_accuracy: 0.4661\n",
      "Epoch 25/48\n",
      "225/225 [==============================] - 704s 3s/step - loss: 1.3234 - accuracy: 0.4882 - val_loss: 1.2706 - val_accuracy: 0.5236\n",
      "Epoch 26/48\n",
      "225/225 [==============================] - 707s 3s/step - loss: 1.3204 - accuracy: 0.4926 - val_loss: 1.1406 - val_accuracy: 0.5713\n",
      "Epoch 27/48\n",
      "225/225 [==============================] - 701s 3s/step - loss: 1.3148 - accuracy: 0.4983 - val_loss: 1.1687 - val_accuracy: 0.5604\n",
      "Epoch 28/48\n",
      "225/225 [==============================] - 707s 3s/step - loss: 1.2984 - accuracy: 0.4997 - val_loss: 1.1759 - val_accuracy: 0.5456\n",
      "Epoch 29/48\n",
      "225/225 [==============================] - 694s 3s/step - loss: 1.2939 - accuracy: 0.5091 - val_loss: 1.1870 - val_accuracy: 0.5429\n",
      "Epoch 30/48\n",
      "225/225 [==============================] - 680s 3s/step - loss: 1.2858 - accuracy: 0.5083 - val_loss: 1.2338 - val_accuracy: 0.5446\n",
      "Epoch 31/48\n",
      "225/225 [==============================] - 676s 3s/step - loss: 1.2796 - accuracy: 0.5102 - val_loss: 1.1014 - val_accuracy: 0.5788\n",
      "Epoch 32/48\n",
      "225/225 [==============================] - 684s 3s/step - loss: 1.2737 - accuracy: 0.5126 - val_loss: 1.1318 - val_accuracy: 0.5706\n",
      "Epoch 33/48\n",
      "225/225 [==============================] - 689s 3s/step - loss: 1.2678 - accuracy: 0.5119 - val_loss: 1.1230 - val_accuracy: 0.5599\n",
      "Epoch 34/48\n",
      "225/225 [==============================] - 696s 3s/step - loss: 1.2625 - accuracy: 0.5201 - val_loss: 1.2161 - val_accuracy: 0.5419\n",
      "Epoch 35/48\n",
      "225/225 [==============================] - 692s 3s/step - loss: 1.2573 - accuracy: 0.5214 - val_loss: 1.0441 - val_accuracy: 0.6077\n",
      "Epoch 36/48\n",
      "225/225 [==============================] - 683s 3s/step - loss: 1.2528 - accuracy: 0.5207 - val_loss: 1.1630 - val_accuracy: 0.5517\n",
      "Epoch 37/48\n",
      "225/225 [==============================] - 679s 3s/step - loss: 1.2464 - accuracy: 0.5240 - val_loss: 1.1542 - val_accuracy: 0.5662\n",
      "Epoch 38/48\n",
      "225/225 [==============================] - 678s 3s/step - loss: 1.2354 - accuracy: 0.5244 - val_loss: 1.3292 - val_accuracy: 0.5068\n",
      "Epoch 39/48\n",
      "225/225 [==============================] - 699s 3s/step - loss: 1.2382 - accuracy: 0.5279 - val_loss: 1.2041 - val_accuracy: 0.5440\n",
      "Epoch 40/48\n",
      "225/225 [==============================] - 930s 4s/step - loss: 1.2378 - accuracy: 0.5285 - val_loss: 1.3181 - val_accuracy: 0.4845\n",
      "Epoch 41/48\n",
      "225/225 [==============================] - 1237s 5s/step - loss: 1.2301 - accuracy: 0.5293 - val_loss: 1.0570 - val_accuracy: 0.6084\n",
      "Epoch 42/48\n",
      "225/225 [==============================] - 818s 4s/step - loss: 1.2217 - accuracy: 0.5349 - val_loss: 1.1083 - val_accuracy: 0.5803\n",
      "Epoch 43/48\n",
      "225/225 [==============================] - 726s 3s/step - loss: 1.2228 - accuracy: 0.5346 - val_loss: 1.2601 - val_accuracy: 0.5205\n",
      "Epoch 44/48\n",
      "225/225 [==============================] - 726s 3s/step - loss: 1.2149 - accuracy: 0.5347 - val_loss: 1.0999 - val_accuracy: 0.5773\n",
      "Epoch 45/48\n",
      "225/225 [==============================] - 959s 4s/step - loss: 1.2114 - accuracy: 0.5396 - val_loss: 1.1750 - val_accuracy: 0.5510\n",
      "Epoch 46/48\n",
      "225/225 [==============================] - 680s 3s/step - loss: 1.2080 - accuracy: 0.5392 - val_loss: 1.2936 - val_accuracy: 0.5081\n",
      "Epoch 47/48\n",
      "225/225 [==============================] - 695s 3s/step - loss: 1.2030 - accuracy: 0.5396 - val_loss: 1.1530 - val_accuracy: 0.5607\n",
      "Epoch 48/48\n",
      "225/225 [==============================] - 1052s 5s/step - loss: 1.2053 - accuracy: 0.5435 - val_loss: 1.1992 - val_accuracy: 0.5398\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator=train_set,\n",
    "                                steps_per_epoch=train_set//train_set.batch_size,\n",
    "                                epochs=epochs,\n",
    "                                validation_data = val_set,\n",
    "                                validation_steps = val_set//val_set.batch_size,                               \n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2d947a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('face_exp_model_inception.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e61621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier(r'C:\\Users\\harsh\\Documents\\MASK DETECT\\haarcascade_frontalface_default.xml')\n",
    "classifier = tf.keras.models.load_model(\"face_exp_model_inception.h5\")\n",
    "\n",
    "class_labels = ['Angry','Disgust','Fear','Happy','Neutral','Sad','Surprise']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = cap.read()\n",
    "    labels = []\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray,1.3,5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h,x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "\n",
    "        if np.sum([roi_gray])!=0:\n",
    "            roi = roi_gray.astype('float')/255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi,axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "\n",
    "            preds = classifier.predict(roi)[0]\n",
    "            label=class_labels[preds.argmax()]\n",
    "            label_position = (x,y)\n",
    "            cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(255,255,0),3)\n",
    "\n",
    "    cv2.imshow('Emotion Detector',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40acb854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3072662a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aec705b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
